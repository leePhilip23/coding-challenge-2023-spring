# ACM Research coding challenge (Spring 2023)

For this coding project I analyzed the data and found that there needs to be some errors that needed to be fixed. For example, in the 'Star color' column of the dataset, there were duplicate types of data (for example: 'red' and 'Red') that needed to addressed. I also used one-hot-encoding for the categrocial data except for 'Star type' since 'Star Type' is a particular output. Once I organized and cleaned the dataset, I made a histograms to determine specific features and also to see if there's an unbalance of data that could possibly affect the ML models. After looking at many graphs, I came to the conclusion that every feature could be used, and I did a 80:20 split on my data. I then used four differenty type of ML models for scikit-learn to determine y_predicted values as well as the accuracy of my algorithms. I found that on average a random forest worked the best, second place was a decision tree, third was a multinomial naive bayes, and support vector machines came in last place. For naive bayes, I also had to normalize the data so that it could predict even with negative values as part of my input. This also helped in the case that my naive bayes was working better when the input values are closer to each other.

Works Cited:
https://stackoverflow.com/questions/24169238/dealing-with-negative-values-in-sklearn-multinomialnb -> This was used to create a pipeline that normalizes data before putting it in naive bayes.
